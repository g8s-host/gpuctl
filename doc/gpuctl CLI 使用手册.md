# gpuctl CLI ä½¿ç”¨æ‰‹å†Œ

## 1. äº§å“ç®€ä»‹

gpuctl æ˜¯é¢å‘ç®—æ³•å·¥ç¨‹å¸ˆçš„ AI ç®—åŠ›è°ƒåº¦å¹³å°å‘½ä»¤è¡Œå·¥å…·ï¼Œè®©æ‚¨æ— éœ€æŒæ¡ Kubernetes ç­‰åº•å±‚åŸºç¡€è®¾æ–½çŸ¥è¯†ï¼Œå³å¯é«˜æ•ˆæäº¤å’Œç®¡ç† AI è®­ç»ƒä¸æ¨ç†ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹æ€§

- âˆ™ğŸš€ **ç®€å•æ˜“ç”¨**ï¼šå£°æ˜å¼ YAML é…ç½®ï¼Œç›´è§‚çš„ CLI å‘½ä»¤
- âˆ™âš¡ **é«˜æ€§èƒ½**ï¼šæ·±åº¦ä¼˜åŒ– Deepspeedã€VLLM ç­‰ä¸»æµå·¥å…·æ€§èƒ½
- âˆ™ğŸ”§ **å·¥å…·å…¼å®¹**ï¼šå…¨é¢æ”¯æŒ Llama Factoryã€SGLang ç­‰ AI å·¥å…·é“¾
- âˆ™ğŸ“Š **èµ„æºå¯è§†**ï¼šå®æ—¶ç›‘æ§ GPU åˆ©ç”¨ç‡ã€è®­ç»ƒè¿›åº¦ç­‰å…³é”®æŒ‡æ ‡
- âˆ™ğŸ”’ **èµ„æºéš”ç¦»**ï¼šåŸºäºèµ„æºæ± çš„ç²¾ç»†åŒ–ç®¡ç†ï¼Œé¿å…ä»»åŠ¡äº‰æŠ¢

## 2. å®‰è£…ä¸é…ç½®

### 2.1 å®‰è£… gpuctl

```
# ä¸‹è½½æœ€æ–°ç‰ˆæœ¬ (ç¤ºä¾‹ç‰ˆæœ¬å·)
wget https://download.example.com/gpuctl/gpuctl-v1.0.0-linux-amd64 -O /usr/local/bin/gpuctl

# æ·»åŠ æ‰§è¡Œæƒé™
chmod +x /usr/local/bin/gpuctl

# éªŒè¯å®‰è£…
gpuctl version

# é¢„æœŸè¾“å‡ºï¼š
# gpuctl version v1.0.0
# Build Date: 2024-06-01
# Git Commit: a1b2c3d4
# Platform: linux/amd64
```

### 2.2 é…ç½®è®¤è¯

```
# é…ç½® API æœåŠ¡å™¨åœ°å€å’Œè®¤è¯ä»¤ç‰Œ
gpuctl config set-context production \
  --server=https://gpuctl.example.com \
  --token=your-bearer-token-here

# æŸ¥çœ‹å½“å‰é…ç½®
gpuctl config view

# é¢„æœŸè¾“å‡ºï¼š
# CURRENT CONTEXT: production
# SERVER: https://gpuctl.example.com
# TOKEN: ************abcd
# USER: alice@example.com
# NAMESPACE: default
```

### 2.3 éªŒè¯è¿æ¥

```
# æµ‹è¯•ä¸å¹³å°çš„è¿æ¥çŠ¶æ€
gpuctl cluster-info

# é¢„æœŸè¾“å‡ºï¼š
# Cluster: production
# Server Version: v1.0.0
# API Server: https://gpuctl.example.com
# Status: Connected âœ“

# é¢„æœŸè¾“å‡ºï¼š
Cluster: production
Server Version: v1.0.0
API Server: https://gpuctl.example.com
Platform Status: Healthy âœ“
GPU Nodes: 8 nodes (64 GPUs total)
Scheduler: Running
Last Heartbeat: 2024-06-01 10:30:00 UTC
```

## 3. å¿«é€Ÿå¼€å§‹

### 3.1 æ‚¨çš„ç¬¬ä¸€ä¸ªè®­ç»ƒä»»åŠ¡

**æ­¥éª¤ 1ï¼šåˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶** (`qwen-sft.yaml`)

```
kind: training
version: v0.1

job:
  name: my-first-llama-training
  epochs: 3
  batch_size: 8
  priority: medium

environment:
  image: registry.example.com/llama-factory-deepspeed:v0.8.0
  command: ["llama-factory-cli", "train", "--stage", "sft", "--model_name_or_path", "/models/qwen2-7b", "--dataset", "alpaca-qwen", "--output_dir", "/output"]

resources:
#  pool: training-pool
  gpu: 2
  cpu: 16
  memory: 64Gi
```

**æ­¥éª¤ 2ï¼šæäº¤ä»»åŠ¡**

```
gpuctl create -f qwen-sft.yaml

# é¢„æœŸè¾“å‡ºï¼š
âœ… Job created successfully!
Job ID: my-first-llama-training-abc123
Name: my-first-llama-training
Kind: training
Status: Pending
Pool: training-pool
Estimated Start: within 30 seconds
View details: gpuctl describe job my-first-llama-training-abc123
```



**æ­¥éª¤ 3ï¼šæŸ¥çœ‹ä»»åŠ¡çŠ¶æ€**

```
# æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨
gpuctl get jobs

# é¢„æœŸè¾“å‡ºï¼š
# NAME                      KIND       POOL           STATUS    GPU   PROGRESS  AGE
# my-first-llama-training   training   training-pool  running   2     0%        30s
```



**æ­¥éª¤ 4ï¼šå®æ—¶ç›‘æ§è¿›åº¦**

```
# æŸ¥çœ‹ä»»åŠ¡è¯¦æƒ…
gpuctl describe job my-first-llama-training
```

```
# é¢„æœŸè¾“å‡ºï¼š
Job Details: my-first-llama-training-abc123
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Basic Information:
  Name: my-first-llama-training
  ID: my-first-llama-training-abc123
  Kind: training
  Status: Running âœ“
  Priority: medium
  Created: 2024-06-01 10:00:00 UTC (2 minutes ago)
  Started: 2024-06-01 10:00:30 UTC

Resource Configuration:
  Pool: training-pool
  GPU: 2 x A100-80G
  CPU: 16 cores
  Memory: 64 GiB
  Node: gpu-node-3

Current Metrics:
  GPU Utilization: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 89.2%
  Memory Usage: 58 GiB / 80 GiB (72.5%)
  Training Progress: â–ˆâ–ˆâ–ˆâ–ˆâˆ™âˆ™âˆ™âˆ™âˆ™âˆ™ 15.3%
  Throughput: 245 tokens/second
  Current Epoch: 1/3
  Steps: 150/980

Associated Pods:
  â€¢ my-first-llama-training-abc123-pod-1 (Running)
  â€¢ my-first-llama-training-abc123-pod-2 (Running)

Next Steps:
  View logs: gpuctl logs my-first-llama-training-abc123 -f
  Monitor: watch gpuctl describe job my-first-llama-training-abc123
```



```
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
gpuctl logs  -f my-first-llama-training
```

```
[2024-06-01 10:01:30] INFO: Starting training with 2 GPUs
[2024-06-01 10:01:31] INFO: Using DeepSpeed ZeRO-2 optimization
[2024-06-01 10:01:35] INFO: Epoch 1/3, Step 10/980, Loss: 2.345, LR: 2.00e-05
[2024-06-01 10:02:15] INFO: Epoch 1/3, Step 20/980, Loss: 1.987, LR: 2.00e-05
[2024-06-01 10:02:55] INFO: Epoch 1/3, Step 30/980, Loss: 1.734, LR: 2.00e-05
...
```



## 4. æ ¸å¿ƒå‘½ä»¤è¯¦è§£

### 4.1 ä»»åŠ¡ç®¡ç†å‘½ä»¤

#### åˆ›å»ºä»»åŠ¡

```
# åˆ›å»ºå•ä¸ªä»»åŠ¡
gpuctl create -f training-job.yaml

# é¢„æœŸè¾“å‡ºï¼š
âœ… Job created successfully!
Job ID: custom-training-xyz789
Name: custom-training
Kind: training
Status: Pending
Pool: training-pool
Estimated Start: within 45 seconds
View details: gpuctl describe job custom-training-xyz789


# æ‰¹é‡åˆ›å»ºå¤šä¸ªä»»åŠ¡
gpuctl create -f task1.yaml -f task2.yaml -f task3.yaml

# é¢„æœŸè¾“å‡ºï¼š
ğŸ”„ Creating 3 jobs...
âœ… task1-experiment-001: Created successfully
âœ… task2-experiment-002: Created successfully  
âœ… task3-experiment-003: Created successfully

Summary:
â€¢ Created: 3 jobs
â€¢ Pending: 3 jobs
â€¢ Failed: 0 jobs
View all jobs: gpuctl get jobs --name task1,task2,task3
```

#### æŸ¥çœ‹ä»»åŠ¡

```
# æŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡
gpuctl get jobs

# é¢„æœŸè¾“å‡ºï¼š
NAME                          KIND       POOL           STATUS    GPU   PROGRESS  AGE
my-first-llama-training       training   training-pool  running   2     15%       2m
qwen2-7b-sft-xyz789           training   training-pool  completed 4     100%      1h


# æŒ‰èµ„æºæ± ç­›é€‰ä»»åŠ¡
gpuctl get jobs --pool training-pool

# é¢„æœŸè¾“å‡ºï¼š
NAME                          KIND       POOL           STATUS    GPU   PROGRESS  AGE
my-first-llama-training       training   training-pool  running   2     15%       2m
qwen2-7b-sft-xyz789           training   training-pool  completed 4     100%      1h

# æŒ‰ä»»åŠ¡ç±»å‹ç­›é€‰
gpuctl get jobs --kind training
NAME                          KIND       POOL           STATUS    GPU   PROGRESS  AGE
my-first-llama-training       training   training-pool  running   2     15%       2m
qwen2-7b-sft-xyz789           training   training-pool  completed 4     100%      1h

gpuctl get jobs --kind inference
gpuctl get jobs --kind notebook

# æŒ‰çŠ¶æ€ç­›é€‰
gpuctl get jobs --status running
gpuctl get jobs --status pending
gpuctl get jobs --status completed
gpuctl get jobs --status failed

# ç»„åˆç­›é€‰æ¡ä»¶
gpuctl get jobs --pool training-pool --status running --kind training

# è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼
gpuctl get jobs -o wide        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
gpuctl get jobs -o yaml        # YAMLæ ¼å¼è¾“å‡º
gpuctl get jobs --sort-by=age  # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
gpuctl get jobs --sort-by=gpu  # æŒ‰GPUæ•°é‡æ’åº

# æŒç»­æŸ¥çœ‹
watch gpuctl get jobs -o wide
watch gpuctl get jobs
```

#### ä»»åŠ¡è¯¦æƒ…ä¸ç›‘æ§

```
# æŸ¥çœ‹ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
gpuctl describe job <job-id>

# é¢„æœŸè¾“å‡ºï¼š
Job Details: my-first-llama-training-abc123
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Basic Information:
  Name: my-first-llama-training
  ID: my-first-llama-training-abc123
  Kind: training
  Status: Running âœ“
  Priority: medium
  Created: 2024-06-01 10:00:00 UTC (2 minutes ago)
  Started: 2024-06-01 10:00:30 UTC

Resource Configuration:
  Pool: training-pool
  GPU: 2 x A100-80G
  CPU: 16 cores
  Memory: 64 GiB
  Node: gpu-node-3

Current Metrics:
  GPU Utilization: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 89.2%
  Memory Usage: 58 GiB / 80 GiB (72.5%)
  Training Progress: â–ˆâ–ˆâ–ˆâ–ˆâˆ™âˆ™âˆ™âˆ™âˆ™âˆ™ 15.3%
  Throughput: 245 tokens/second
  Current Epoch: 1/3
  Steps: 150/980

Associated Pods:
  â€¢ my-first-llama-training-abc123-pod-1 (Running)
  â€¢ my-first-llama-training-abc123-pod-2 (Running)

Next Steps:
  View logs: gpuctl logs my-first-llama-training-abc123 -f
  Monitor: watch gpuctl describe job my-first-llama-training-abc123

```

```
# å®æ—¶æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
gpuctl logs <job-id> -f

# é¢„æœŸè¾“å‡ºï¼š
[2024-06-01 10:01:30] INFO: Starting training with 2 GPUs
[2024-06-01 10:01:31] INFO: Using DeepSpeed ZeRO-2 optimization
[2024-06-01 10:01:35] INFO: Epoch 1/3, Step 10/980, Loss: 2.345, LR: 2.00e-05
[2024-06-01 10:02:15] INFO: Epoch 1/3, Step 20/980, Loss: 1.987, LR: 2.00e-05
[2024-06-01 10:02:55] INFO: Epoch 1/3, Step 30/980, Loss: 1.734, LR: 2.00e-05
...

```



```

# æŸ¥çœ‹æœ€è¿‘100è¡Œæ—¥å¿—
gpuctl logs <job-id> --tail=100

# é¢„æœŸè¾“å‡ºï¼š
=== Last 100 lines of logs ===
[2024-06-01 10:05:30] INFO: Epoch 1/3, Step 80/980, Loss: 1.234, LR: 2.00e-05
[2024-06-01 10:06:10] INFO: Epoch 1/3, Step 90/980, Loss: 1.198, LR: 2.00e-05
[2024-06-01 10:06:50] INFO: Epoch 1/3, Step 100/980, Loss: 1.165, LR: 2.00e-05
...

# æŒ‰æ—¶é—´èŒƒå›´æŸ¥çœ‹æ—¥å¿—
gpuctl logs <job-id> --since=1h
gpuctl logs <job-id> --since-time="2024-01-01T10:00:00Z"

# æ—¥å¿—å…³é”®è¯è¿‡æ»¤
gpuctl logs <job-id> | grep "ERROR"
gpuctl logs <job-id> | grep -i "epoch"
```



#### ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†

```
# æš‚åœè¿è¡Œä¸­çš„ä»»åŠ¡ï¼ˆä¿ç•™èµ„æºï¼‰
gpuctl pause job <job-id>

# æ¢å¤æš‚åœçš„ä»»åŠ¡
gpuctl resume job <job-id>

# åˆ é™¤ä»»åŠ¡
gpuctl delete job <job-id>



# å¼ºåˆ¶åˆ é™¤ï¼ˆç«‹å³é‡Šæ”¾èµ„æºï¼‰
gpuctl delete job <job-id> --force

# æ‰¹é‡åˆ é™¤ä»»åŠ¡
gpuctl delete job job1 job2 job3

# é€šè¿‡é…ç½®æ–‡ä»¶åˆ é™¤
gpuctl delete -f job.yaml

# é¢„æœŸè¾“å‡ºï¼š
ğŸ—‘ï¸  Deleting job: my-first-llama-training-abc123
âš ï¸  This will terminate the training process and release all resources
â“ Are you sure you want to continue? [y/N]: y
ğŸ”„ Stopping training process...
ğŸ”„ Cleaning up temporary files...
âœ… Job deleted successfully
Released resources: 2 GPUs, 16 CPU cores, 64Gi memory
```

### 4.2 èµ„æºæ± ç®¡ç†



#### èµ„æºæ± æ“ä½œï¼ˆç®¡ç†å‘˜æƒé™ï¼‰

```
# åˆ›å»ºèµ„æºæ± 
gpuctl create -f <your-pool-name>.yaml
âœ… Resource pool created successfully!
Name: training-pool
Description: é«˜æ€§èƒ½è®­ç»ƒèµ„æºæ± ï¼Œç”¨äºå¤§æ¨¡å‹è®­ç»ƒä»»åŠ¡

Status: Active
Nodes: 0 nodes (0 GPUs) - Use 'gpuctl add node' to assign nodes
View details: gpuctl describe pool training-pool
```



```
# åˆ é™¤èµ„æºæ± 
gpuctl delete -f <your-pool-name>.yaml
ğŸ—‘ï¸  Deleting resource pool: training-pool
âš ï¸  This action cannot be undone. The following will be affected:
â€¢ 8 nodes will be removed from the pool
â€¢ 32 running jobs will be moved to default-pool
#â€¢ Resource quotas will be removed

â“ Are you sure you want to continue? [y/N]: y
ğŸ”„ Moving running jobs to default-pool...
ğŸ”„ Removing node labels...
ğŸ”„ Cleaning up pool configuration...
âœ… Resource pool 'training-pool' deleted successfully
#Released: 8 nodes, 64 GPUs
Affected jobs: 32 jobs moved to default-pool
```





#### æŸ¥çœ‹èµ„æºæ± 

```
# æŸ¥çœ‹æ‰€æœ‰èµ„æºæ± åŠèµ„æºä½¿ç”¨æƒ…å†µ
gpuctl get pools

# é¢„æœŸè¾“å‡ºï¼š
POOL NAME         TYPE       NODES  GPU_TOTAL  GPU_USED  GPU_FREE  UTILIZATION  STATUS    AGE
training-pool     training   4      32         16        16        50%          âœ… Active   30d
inference-pool    inference  2      16         8         8         50%          âœ… Active   30d
dev-pool          development 2     8          4         4         50%          âœ… Active   15d
experiment-pool   research   2      16         0         16        0%           âœ… Active   7d
default-pool      mixed      0      0          0         0         0%           âœ… Active   30d

ğŸ’¡ Use 'gpuctl describe pool <name>' for detailed information
```



```
# æŸ¥çœ‹ç‰¹å®šèµ„æºæ± è¯¦æƒ…
gpuctl describe pool training-pool

# é¢„æœŸè¾“å‡ºï¼š
Resource Pool: training-pool
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Description: é«˜æ€§èƒ½è®­ç»ƒèµ„æºæ± ï¼Œç”¨äºå¤§æ¨¡å‹è®­ç»ƒä»»åŠ¡
Type: Training
Status: Active âœ…
Created: 2024-05-01 10:00:00 UTC (30 days ago)
Updated: 2024-06-01 09:00:00 UTC (1 hour ago)

Resource Configuration:
â€¢ Total Nodes: 4 nodes
â€¢ Total GPU: 32 (16 used, 16 free)
â€¢ GPU Types: A100-80G (24), A100-40G (8)
â€¢ Total CPU: 256 cores
â€¢ Total Memory: 1 TiB

Current Utilization:
â€¢ GPU Usage: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâˆ™ 50.0%
â€¢ Active Jobs: 12 jobs
â€¢ Avg GPU Utilization: 78.3%
â€¢ Peak Utilization: 92.1% (2024-05-15 14:30:00)

Quota Limits:
â€¢ Max Jobs: 50
â€¢ Max GPU per Job: 8
â€¢ Max Concurrent Users: 20
â€¢ Preemption: Allowed for high priority jobs

Associated Nodes (4):
â€¢ gpu-node-1: 8 GPUs (4 used) - A100-80G
â€¢ gpu-node-2: 8 GPUs (4 used) - A100-80G
â€¢ gpu-node-3: 8 GPUs (4 used) - A100-80G
â€¢ gpu-node-4: 8 GPUs (4 used) - A100-40G

Active Jobs (12):
â€¢ qwen2-7b-sft-abc123 (4 GPUs, 45% progress, high priority)
â€¢ llama3-training-def456 (2 GPUs, 78% progress, medium priority)
â€¢ ...

```

```
# æŸ¥çœ‹èµ„æºæ± ä¸­çš„ä»»åŠ¡
gpuctl get jobs --pool training-pool

# é¢„æœŸè¾“å‡ºï¼š
NAME                          KIND       POOL           STATUS    GPU   PROGRESS  AGE
my-first-llama-training       training   training-pool  running   2     15%       2m
qwen2-7b-sft-xyz789           training   training-pool  completed 4     100%      1h
```



### 4.3 èŠ‚ç‚¹ç®¡ç†

#### æŸ¥çœ‹èŠ‚ç‚¹ä¿¡æ¯

```
# æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹
gpuctl get nodes

# æŸ¥çœ‹èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
gpuctl describe node <node-name>

# æŒ‰èµ„æºæ± æŸ¥çœ‹èŠ‚ç‚¹
gpuctl get nodes --pool training-pool

# æŒ‰GPUç±»å‹æŸ¥çœ‹èŠ‚ç‚¹
gpuctl get nodes --gpu-type a100-80g

# æŸ¥çœ‹èŠ‚ç‚¹GPUè¯¦æƒ…
gpuctl get nodes --gpu-detail

# æŸ¥çœ‹èŠ‚ç‚¹æ ‡ç­¾
gpuctl get node-labels --all
```

#### èŠ‚ç‚¹æ ‡ç­¾ç®¡ç†ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰

```
# ç»™èŠ‚ç‚¹æ·»åŠ æ ‡ç­¾
gpuctl label node node-1 nvidia.com/gpu-type=a100-80g

# æ‰¹é‡æ·»åŠ æ ‡ç­¾
gpuctl label node node-2 node-3 company.com/gpu-model=a100-40g

# è¦†ç›–ç°æœ‰æ ‡ç­¾
gpuctl label node node-1 nvidia.com/gpu-type=a100-40g --overwrite

# æŸ¥çœ‹ç‰¹å®šæ ‡ç­¾
gpuctl get node-labels node-1 --key=nvidia.com/gpu-type

# åˆ é™¤æ ‡ç­¾
gpuctl label node node-1 nvidia.com/gpu-type --delete
```



## 2. èŠ‚ç‚¹ç®¡ç†å‘½ä»¤

### æ·»åŠ èŠ‚ç‚¹åˆ°èµ„æºæ± 

```
# æ·»åŠ å•ä¸ªèŠ‚ç‚¹åˆ°èµ„æºæ± 
gpuctl add node gpu-node-1 --pool training-pool

# æ‰¹é‡æ·»åŠ å¤šä¸ªèŠ‚ç‚¹
gpuctl add node gpu-node-2 gpu-node-3 gpu-node-4 --pool training-pool

# æ·»åŠ èŠ‚ç‚¹å¹¶æŒ‡å®šGPUç±»å‹
gpuctl add node gpu-node-5  --gpu-type A100-80G --pool training-pool
```

**è¿”å›ç¤ºä¾‹ï¼š**

```
ğŸ”§ Adding nodes to training-pool...
âœ… gpu-node-1: Successfully added (8 x A100-80G GPUs)
âœ… gpu-node-2: Successfully added (8 x A100-80G GPUs)  
âœ… gpu-node-3: Successfully added (8 x A100-80G GPUs)
âœ… gpu-node-4: Successfully added (8 x A100-40G GPUs)

Summary:
â€¢ Added: 4 nodes
â€¢ Total GPU: 32 GPUs (24 x A100-80G, 8 x A100-40G)
â€¢ Pool Status: Active with 32/32 GPUs available
â€¢ Next: Submit jobs using 'gpuctl create -f job.yaml'

Updated Pool Status:
training-pool: 4 nodes, 32 GPUs, 0% utilization
```

### ä»èµ„æºæ± ç§»é™¤èŠ‚ç‚¹

```
# ä»èµ„æºæ± ç§»é™¤å•ä¸ªèŠ‚ç‚¹
gpuctl remove node gpu-node-3 --pool training-pool

# æ‰¹é‡ç§»é™¤å¤šä¸ªèŠ‚ç‚¹
gpuctl remove node gpu-node-4 gpu-node-5 --pool training-pool

# å¼ºåˆ¶ç§»é™¤ï¼ˆå³ä½¿èŠ‚ç‚¹ä¸Šæœ‰è¿è¡Œçš„ä»»åŠ¡ï¼‰
gpuctl remove node gpu-node-1 --pool training-pool --force
```

**è¿”å›ç¤ºä¾‹ï¼š**

```
ğŸ”§ Removing nodes from training-pool...
âš ï¸  gpu-node-3 has 2 running jobs (using 4 GPUs):
   â€¢ job-abc123 (2 GPUs, training, 45% progress)
   â€¢ job-def456 (2 GPUs, training, 15% progress)

Summary:
â€¢ Removed: 1 node (gpu-node-3)
â€¢ GPUs Removed: 8 x A100-80G
â€¢ Jobs Affected: 2 jobs completed normally
â€¢ Pool Status: training-pool now has 24 GPUs (75% capacity)
```

**å¼ºåˆ¶ç§»é™¤çš„è¿”å›ç¤ºä¾‹ï¼š**

```
ğŸ”§ Force removing node from training-pool...
ğŸš¨ FORCE REMOVAL: This will terminate all running jobs on gpu-node-1
Running jobs to be terminated:
â€¢ job-abc123 (4 GPUs, training, 60% progress) - WILL BE LOST
â€¢ job-def456 (2 GPUs, training, 30% progress) - WILL BE LOST

â“ Are you absolutely sure? This cannot be undone. [y/N]: y

ğŸ”„ Force removing gpu-node-1...
ğŸ”„ Terminating 2 running jobs...
ğŸ”„ Removing node labels...
âœ… gpu-node-1: Force removed from training-pool

Summary:
â€¢ Removed: 1 node (gpu-node-1) 
â€¢ GPUs Removed: 8 x A100-80G
â€¢ Jobs Terminated: 2 jobs (6 GPUs total)
â€¢ Data Loss: Training progress from terminated jobs is not recoverable
â€¢ Pool Status: training-pool now has 24 GPUs
```

### æŸ¥çœ‹èµ„æºæ± èŠ‚ç‚¹åˆ—è¡¨

```
# åˆ—å‡ºæŒ‡å®šèµ„æºæ± çš„æ‰€æœ‰èŠ‚ç‚¹
gpuctl get nodes --pool training-pool
```

**è¿”å›ç¤ºä¾‹ï¼š**

```
NODE NAME     STATUS   GPU_TOTAL  GPU_USED  GPU_FREE  GPU_TYPE    UTILIZATION  JOBS  AGE
gpu-node-1    Ready    8          4         4         A100-80G   85%          2    30d
gpu-node-2    Ready    8          4         4         A100-80G   78%          3    30d
gpu-node-3    Ready    8          4         4         A100-80G   92%          1    30d
gpu-node-4    Ready    8          4         4         A100-40G   65%          2    30d

TOTAL: 4 nodes, 32 GPUs (16 used, 16 free) - 50.0% utilization
# è¯¦ç»†æŸ¥çœ‹èµ„æºæ± èŠ‚ç‚¹ä¿¡æ¯
gpuctl get nodes --pool training-pool -o wide
```

**è¿”å›ç¤ºä¾‹ï¼š**

```
NODE NAME     STATUS   GPU_TOTAL  GPU_USED  GPU_FREE  GPU_TYPE    CPU  MEMORY   JOBS  UTILIZATION  TEMPERATURE  AGE
gpu-node-1    Ready    8          4         4         A100-80G   64   256Gi    2     85%          72Â°C         30d
gpu-node-2    Ready    8          4         4         A100-80G   64   256Gi    3     78%          68Â°C         30d
gpu-node-3    Ready    8          4         4         A100-80G   64   256Gi    1     92%          75Â°C         30d
gpu-node-4    Ready    8          4         4         A100-40G   64   256Gi    2     65%          62Â°C         30d

SUMMARY:
â€¢ Nodes: 4 (all Ready)
â€¢ GPUs: 32 total (16 used, 16 free) - 50.0% utilization
â€¢ Jobs: 8 running jobs
â€¢ Avg Utilization: 80.0%
â€¢ Health: All nodes operating within normal parameters
```

## 5. å®ç”¨åœºæ™¯ç¤ºä¾‹

### 5.1 å¤§æ¨¡å‹å¾®è°ƒå®Œæ•´æµç¨‹

```
# 1. æäº¤å¾®è°ƒä»»åŠ¡
gpuctl create -f qwen2-7b-sft.yaml

# 2. ç›‘æ§ä»»åŠ¡å¯åŠ¨
gpuctl get jobs --name qwen2-7b-sft -w

# 3. å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
gpuctl logs -f qwen2-7b-sft-xxxxx

# 4. ç›‘æ§è®­ç»ƒæŒ‡æ ‡
watch gpuctl describe job qwen2-7b-sft-xxxxx

# 5. è®­ç»ƒå®Œæˆåä¸‹è½½ç»“æœ
gpuctl cp qwen2-7b-sft-xxxxx:/output ./training-results/

# 6. åœæ­¢è®­ç»ƒä»»åŠ¡
gpuctl apply -f qwen2-7b-sft.yaml
```

### 5.2 å¤šå®éªŒå¯¹æ¯”

```
# 1. å‡†å¤‡å¤šä¸ªå®éªŒé…ç½®
gpuctl create -f exp1.yaml exp2.yaml exp3.yaml

# 2. ç›‘æ§æ‰€æœ‰å®éªŒè¿›åº¦
watch 'gpuctl get jobs --pool experiment-pool'

# 3. æ¯”è¾ƒå®éªŒèµ„æºä½¿ç”¨
gpuctl get jobs --pool experiment-pool -o wide

# 4. æ‰¹é‡ç®¡ç†å®éªŒä»»åŠ¡
# æš‚åœæ‰€æœ‰å®éªŒ
gpuctl get jobs --pool experiment-pool --status running -o name | xargs -I {} gpuctl pause job {}

# åˆ é™¤å¤±è´¥çš„ä»»åŠ¡
gpuctl get jobs --pool experiment-pool --status failed -o name | xargs -I {} gpuctl delete job {}
```

### 5.3 äº¤äº’å¼å¼€å‘

```
# 1. å¯åŠ¨Notebookç¯å¢ƒ
gpuctl create -f notebook.yaml

# 2. è·å–è®¿é—®åœ°å€
gpuctl describe job data-prep-notebook

# 3. åŠ¨æ€è°ƒæ•´èµ„æº
gpuctl scale job data-prep-notebook --gpu=2

# 4. æ–‡ä»¶ä¼ è¾“
gpuctl cp ./local-script.py data-prep-notebook:/home/jovyan/work/

# 5. å…³é—­ç¯å¢ƒ
gpuctl delete job data-prep-notebook
```



## 6. é«˜çº§åŠŸèƒ½

### 6.1 è‡ªåŠ¨è¡¥å…¨é…ç½®

```
# é…ç½®Bashè‡ªåŠ¨è¡¥å…¨
echo 'source <(gpuctl completion bash)' >> ~/.bashrc
source ~/.bashrc

# é…ç½®Zshè‡ªåŠ¨è¡¥å…¨
echo 'source <(gpuctl completion zsh)' >> ~/.zshrc
source ~/.zshrc
```

### 6.2 è¾“å‡ºæ ¼å¼å®šåˆ¶

```
# JSONæ ¼å¼è¾“å‡ºï¼Œä¾¿äºè„šæœ¬å¤„ç†
gpuctl get jobs -o json | jq '.items[] | select(.status == "running")'

# è‡ªå®šä¹‰åˆ—æ˜¾ç¤º
gpuctl get jobs -o custom-columns=NAME:.name,STATUS:.status,GPU:.gpu,POOL:.pool

# å¯¼å‡ºä¸ºYAMLæ–‡ä»¶
gpuctl get job <job-id> -o yaml > job-backup.yaml
```

### 6.3 æ‰¹é‡æ“ä½œæŠ€å·§

```
# ä½¿ç”¨xargsè¿›è¡Œæ‰¹é‡æ“ä½œ
gpuctl get jobs --status completed -o name | xargs -I {} gpuctl delete job {}

# å¹¶è¡Œæäº¤å¤šä¸ªä»»åŠ¡
find ./experiments -name "*.yaml" | xargs -I {} -P 4 gpuctl create -f {}

# æ¡ä»¶æ‰¹é‡æ“ä½œ
gpuctl get jobs --pool training-pool --status running --sort-by=age | \
  tail -n +6 | \  # è·³è¿‡æœ€è¿‘5ä¸ªä»»åŠ¡
  awk '{print $1}' | \
  xargs -I {} gpuctl pause job {}
```

## 7. æ•…éšœæ’æŸ¥ä¸è°ƒè¯•

### 7.1 å¸¸è§é—®é¢˜è¯Šæ–­

```
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
gpuctl cluster-info

# æŸ¥çœ‹å¹³å°ç»„ä»¶çŠ¶æ€
gpuctl get components

# æ£€æŸ¥èµ„æºæ± å¯ç”¨æ€§
gpuctl query pools

# éªŒè¯ä»»åŠ¡é…ç½®
gpuctl create -f job.yaml --dry-run --verbose
```

### 7.2 ä»»åŠ¡è°ƒè¯•æŠ€å·§

```
# æŸ¥çœ‹ä»»åŠ¡äº‹ä»¶ï¼ˆæœ‰åŠ©äºè¯Šæ–­è°ƒåº¦é—®é¢˜ï¼‰
gpuctl describe job <job-id> | grep -A 10 -B 5 Events

# å®æ—¶ç›‘æ§èµ„æºä½¿ç”¨
watch 'gpuctl describe job <job-id> | grep -A 5 "Metrics"'

# è¿›å…¥è°ƒè¯•æ¨¡å¼ï¼ˆå¢åŠ è¯¦ç»†æ—¥å¿—ï¼‰
gpuctl --v=3 create -f job.yaml  # çº§åˆ«1-5ï¼Œæ•°å­—è¶Šå¤§è¶Šè¯¦ç»†
```

### 7.3 è·å–å¸®åŠ©

```
# æŸ¥çœ‹æ‰€æœ‰å‘½ä»¤
gpuctl --help

# æŸ¥çœ‹ç‰¹å®šå‘½ä»¤å¸®åŠ©
gpuctl create --help
gpuctl get --help
gpuctl describe --help

# æŸ¥çœ‹å‘½ä»¤ç”¨æ³•ç¤ºä¾‹
gpuctl examples

# æŸ¥çœ‹ç‰ˆæœ¬ä¿¡æ¯
gpuctl version --client --server
```

## 8. æœ€ä½³å®è·µ

### 8.1 èµ„æºé…ç½®å»ºè®®

```
# è®­ç»ƒä»»åŠ¡æ¨èé…ç½®
resources:
  pool: training-pool
  gpu: 4              # å¤šå¡è®­ç»ƒæå‡æ•ˆç‡
  cpu: 32             # CPUæ ¸å¿ƒæ•°å»ºè®®ä¸ºGPUæ•°çš„8å€
  memory: 128Gi       # å†…å­˜å»ºè®®ä¸ºGPUæ˜¾å­˜çš„1.5å€

# æ¨ç†ä»»åŠ¡æ¨èé…ç½®  
resources:
  pool: inference-pool
  gpu: 1              # å•å¡æ¨ç†ï¼Œé€šè¿‡å‰¯æœ¬æ•°æ‰©å±•
  cpu: 8              # é€‚é‡CPUæ”¯æŒé¢„å¤„ç†
  memory: 32Gi        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´
```

### 8.2 ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†

```
job:
  name: production-training
  priority: high      # ç”Ÿäº§ä»»åŠ¡è®¾ä¸ºé«˜ä¼˜å…ˆçº§
  
job:
  name: experiment-tuning  
  priority: medium    # å®éªŒä»»åŠ¡è®¾ä¸ºä¸­ä¼˜å…ˆçº§

job:
  name: background-processing
  priority: low       # åå°ä»»åŠ¡è®¾ä¸ºä½ä¼˜å…ˆçº§
```

### 8.3 ç›‘æ§ä¸å‘Šè­¦è®¾ç½®

```
# è®¾ç½®èµ„æºä½¿ç”¨é˜ˆå€¼ç›‘æ§
gpuctl get jobs --watch | while read line; do
  if echo "$line" | grep -q "GPU.*9[0-9]%"; then
    echo "é«˜GPUä½¿ç”¨ç‡å‘Šè­¦: $line"
  fi
done

# å®šæœŸæ£€æŸ¥ä»»åŠ¡å¥åº·çŠ¶æ€
while true; do
  gpuctl get jobs --status failed && echo "æœ‰ä»»åŠ¡å¤±è´¥ï¼Œè¯·æ£€æŸ¥"
  sleep 300
done
```

## 9. æŠ€æœ¯æ”¯æŒ

### è·å–å¸®åŠ©

- âˆ™ğŸ“– æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š`gpuctl docs`
- âˆ™ğŸ› æŠ¥å‘Šé—®é¢˜ï¼š`gpuctl bug-report`
- âˆ™ğŸ’¬ ç¤¾åŒºæ”¯æŒï¼šè®¿é—® [ç¤¾åŒºè®ºå›](https://forum.example.com/)

### æ•…éšœåé¦ˆæ¨¡æ¿

```
# ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
gpuctl bug-report --output=diagnostic.tar.gz

# åŒ…å«çš„ä¿¡æ¯ï¼š
# - å®¢æˆ·ç«¯ç‰ˆæœ¬
# - é›†ç¾¤çŠ¶æ€
# - æœ€è¿‘ä»»åŠ¡è®°å½•
# - ç³»ç»Ÿé…ç½®ä¿¡æ¯
```

------

**æ¸©é¦¨æç¤º**ï¼šæœ¬æ‰‹å†Œå†…å®¹ä¼šéšç‰ˆæœ¬æ›´æ–°è€Œè°ƒæ•´ï¼Œè¯·ä½¿ç”¨ `gpuctl docs --latest`è·å–æœ€æ–°æ–‡æ¡£ã€‚ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ ğŸ‰