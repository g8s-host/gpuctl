kind: inference
version: v0.1

# 任务标识
job:
  name: autoscaling-inference
  priority: "high"
  description: "带自动扩缩容的推理任务，用于大语言模型推理服务"

# 环境与镜像
environment:
  image: vllm/vllm-serving:v0.5.0
  command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
  args:
    - "--model"
    - "/models/llama3-8b"
    - "--tensor-parallel-size"
    - "1"
    - "--max-num-seqs"
    - "256"

# 服务配置
service:
  replicas: 2
  port: 8000
  health_check: /health

# 资源规格
resources:
  pool: high-performance-pool
  gpu: 1
  gpu-type: a100-80g
  cpu: 8
  memory: 32Gi
  gpu-share: 2Gi

# 自动扩缩容配置
autoscaling:
  enabled: true
  min_replicas: 2
  max_replicas: 10
  target_gpu_utilization: 70

storage:
  workdirs:
    - path: /models
    - path: /cache